{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec89e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd0111",
   "metadata": {},
   "source": [
    "## 归一化\n",
    "注意不管是LayerNorm还是RMSNorm，求均值、方差等计算方式时，都是针对最后一维度。就相当于有batch_size*seq_len个样本，每个样本要保持均值为0方差为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ff2a6",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "$$\n",
    "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现LayerNorm\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init_()\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = torch.Size(normalized_shape)\n",
    "        self.eps = eps\n",
    "\n",
    "        # 创建可学习的缩放参数gamma和偏移参数beta\n",
    "        # nn.Parameter 会将它们注册为模型的参数，这样在训练时可以被优化器更新\n",
    "        self.gamma = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        dims = tuple(range(x.dim() - len(self.normalized_shape), x.dim()))\n",
    "        print(\"dims:\", dims)\n",
    "        # 计算均值和方差\n",
    "        mean = x.mean(dims, keepdim=True)\n",
    "        var = x.var(dims, keepdim=True, unbiased=False)\n",
    "        # 归一化\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # 缩放和偏移\n",
    "        output = self.gamma * x_normalized + self.beta\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52f838",
   "metadata": {},
   "source": [
    "### RMSNorm\n",
    "$$\n",
    "y = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} \\cdot \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad77c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        # 创建可学习的缩放参数gamma\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # 实现x / sqrt( (1/n) * sum(x_i^2) + eps )\n",
    "        # torch.rsqrt()计算1/sqrt()\n",
    "        rms = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return x * rms\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self._norm(x)\n",
    "        # 缩放gamma\n",
    "        return output * self.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46993501",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d962be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "embedding_dim = x.shape[-1]\n",
    "\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "rms_norm = RMSNorm(embedding_dim)\n",
    "output = rms_norm(x)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caa731",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74ccc7",
   "metadata": {},
   "source": [
    "### GLU\n",
    "$$\n",
    "\\text{GLU}(x, W, V, b, c) = (xW + b)\\otimes \\sigma(xV + c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4d6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int = None, out_features: int = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "\n",
    "        # W与V都是x通过线性变换后得到的；下面两个tensor的bias都设置为了False，是因为假设与LayerNorm联用，LayerNorm本省有bias参数，此外有个x-E[x]的操作\n",
    "        # 导致在GLU中学习/保留bias是多余的\n",
    "        self.wv = nn.Linear(in_features, hidden_features * 2, bias=False)\n",
    "        self.out = nn.Linear(hidden_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        xW ⊗ σ(xV)\n",
    "        '''\n",
    "        content, gate = self.wv(x).chunk(2, dim=-1) # chunk函数将矩阵沿着dim分成2部分\n",
    "        hidden_state = content * torch.sigmoid(gate) # hidden_state.shape = (batch_size, hidden_features)\n",
    "        output = self.out(hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5b010",
   "metadata": {},
   "source": [
    "### SwiGLU\n",
    "$$\n",
    "\\text{SwiGLU}(x, W, V) = (xW) \\otimes \\text{SiLU}(xV)\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\text{SiLU}(x) = x\\cdot\\sigma(x)=\\frac{x}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdee5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int = None, out_features: int = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "\n",
    "        self.w = nn.Linear(in_features, hidden_features)\n",
    "        self.v = nn.Linear(in_features, hidden_features)\n",
    "        self.out = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_for_gate = self.v(x)\n",
    "        gate = x_for_gate * torch.sigmoid(x_for_gate)\n",
    "\n",
    "        content = self.w(x)\n",
    "        hidden_state = content * gate\n",
    "\n",
    "        output = self.out(hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4c88a",
   "metadata": {},
   "source": [
    "### GeGLU\n",
    "$$\n",
    "\\text{GeGLU}(x, W, V) = (xW) \\otimes \\text{GELU}(xV)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17e4b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeGLU(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.wv = nn.Linear(in_features, hidden_features * 2)\n",
    "        self.out = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        content, gate = self.wv(x).chunk(2, dim=-1)\n",
    "        gate = F.gelu(gate)\n",
    "\n",
    "        hidden_state = content * gate\n",
    "\n",
    "        output = self.out(hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f137784",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80160210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 64])\n",
      "torch.Size([2, 10, 64])\n",
      "torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "in_dim = 64\n",
    "x = torch.randn(2, 10, in_dim)\n",
    "activation = GLU(in_features=in_dim, hidden_features=128)\n",
    "\n",
    "output = activation(x)\n",
    "print(output.shape)\n",
    "\n",
    "activation = SwiGLU(in_features=in_dim)\n",
    "output = activation(x)\n",
    "print(output.shape)\n",
    "\n",
    "activation = GeGLU(in_features=in_dim)\n",
    "output = activation(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6efe017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
