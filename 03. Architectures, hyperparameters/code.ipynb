{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec89e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd0111",
   "metadata": {},
   "source": [
    "## 归一化\n",
    "注意不管是LayerNorm还是RMSNorm，求均值、方差等计算方式时，都是针对最后一维度。就相当于有batch_size*seq_len个样本，每个样本要保持均值为0方差为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ff2a6",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "$$\n",
    "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccb1c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现LayerNorm\n",
    "class CustomLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init_()\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = torch.Size(normalized_shape)\n",
    "        self.eps = eps\n",
    "\n",
    "        # 创建可学习的缩放参数gamma和偏移参数beta\n",
    "        # nn.Parameter 会将它们注册为模型的参数，这样在训练时可以被优化器更新\n",
    "        self.gamma = nn.Parameter(torch.ones(self.normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.normalized_shape))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        dims = tuple(range(x.dim() - len(self.normalized_shape), x.dim()))\n",
    "        print(\"dims:\", dims)\n",
    "        # 计算均值和方差\n",
    "        mean = x.mean(dims, keepdim=True)\n",
    "        var = x.var(dims, keepdim=True, unbiased=False)\n",
    "        # 归一化\n",
    "        x_normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        # 缩放和偏移\n",
    "        output = self.gamma * x_normalized + self.beta\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc52f838",
   "metadata": {},
   "source": [
    "### RMSNorm\n",
    "$$\n",
    "y = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} \\cdot \\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad77c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现RMSNorm\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        # 创建可学习的缩放参数gamma\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # 实现x / sqrt( (1/n) * sum(x_i^2) + eps )\n",
    "        # torch.rsqrt()计算1/sqrt()\n",
    "        rms = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        return x * rms\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self._norm(x)\n",
    "        # 缩放gamma\n",
    "        return output * self.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46993501",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d962be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "embedding_dim = x.shape[-1]\n",
    "\n",
    "layer_norm = nn.LayerNorm(embedding_dim)\n",
    "output = layer_norm(x)\n",
    "\n",
    "print(output.shape)\n",
    "\n",
    "rms_norm = RMSNorm(embedding_dim)\n",
    "output = rms_norm(x)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caa731",
   "metadata": {},
   "source": [
    "## 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa74ccc7",
   "metadata": {},
   "source": [
    "### GLU\n",
    "$$\n",
    "\\text{GLU}(x, W, V, b, c) = (xW + b)\\otimes \\sigma(xV + c)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4d6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int = None, out_features: int = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "\n",
    "        # W与V都是x通过线性变换后得到的；下面两个tensor的bias都设置为了False，是因为假设与LayerNorm联用，LayerNorm本省有bias参数，此外有个x-E[x]的操作\n",
    "        # 导致在GLU中学习/保留bias是多余的\n",
    "        self.wv = nn.Linear(in_features, hidden_features * 2, bias=False)\n",
    "        self.out = nn.Linear(hidden_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        xW ⊗ σ(xV)\n",
    "        '''\n",
    "        content, gate = self.wv(x).chunk(2, dim=-1) # chunk函数将矩阵沿着dim分成2部分\n",
    "        hidden_state = content * torch.sigmoid(gate) # hidden_state.shape = (batch_size, hidden_features)\n",
    "        output = self.out(hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca5b010",
   "metadata": {},
   "source": [
    "### SwiGLU\n",
    "$$\n",
    "\\text{SwiGLU}(x, W, V) = (xW) \\otimes \\text{SiLU}(xV)\n",
    "$$\n",
    "其中\n",
    "$$\n",
    "\\text{SiLU}(x) = x\\cdot\\sigma(x)=\\frac{x}{1+e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdee5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, in_features: int, hidden_features: int = None, out_features: int = None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "\n",
    "        self.w = nn.Linear(in_features, hidden_features)\n",
    "        self.v = nn.Linear(in_features, hidden_features)\n",
    "        self.out = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_for_gate = self.v(x)\n",
    "        gate = x_for_gate * torch.sigmoid(x_for_gate)\n",
    "\n",
    "        content = self.w(x)\n",
    "        hidden_state = content * gate\n",
    "\n",
    "        output = self.out(hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4c88a",
   "metadata": {},
   "source": [
    "### GeGLU\n",
    "$$\n",
    "\\text{GeGLU}(x, W, V) = (xW) \\otimes \\text{GELU}(xV)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17e4b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeGLU(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        out_features = out_features or in_features\n",
    "        \n",
    "        self.wv = nn.Linear(in_features, hidden_features * 2)\n",
    "        self.out = nn.Linear(hidden_features, out_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        content, gate = self.wv(x).chunk(2, dim=-1)\n",
    "        gate = F.gelu(gate)\n",
    "\n",
    "        hidden_state = content * gate\n",
    "\n",
    "        output = self.out(hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f137784",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80160210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 64])\n",
      "torch.Size([2, 10, 64])\n",
      "torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "in_dim = 64\n",
    "x = torch.randn(2, 10, in_dim)\n",
    "activation = GLU(in_features=in_dim, hidden_features=128)\n",
    "\n",
    "output = activation(x)\n",
    "print(output.shape)\n",
    "\n",
    "activation = SwiGLU(in_features=in_dim)\n",
    "output = activation(x)\n",
    "print(output.shape)\n",
    "\n",
    "activation = GeGLU(in_features=in_dim)\n",
    "output = activation(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6efe017",
   "metadata": {},
   "source": [
    "### RoPE: rotary position embeddings\n",
    "实现非复数版本RoPE；参考[视频链接](https://www.youtube.com/watch?v=o29P0Kpobz0&t=351s)中的代码实现\n",
    "$$\n",
    "\\theta_j=10000^{-\\frac{2j}{d}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eda3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048, base=1e4):\n",
    "        '''\n",
    "        :dim: 词嵌入的维度，必须是偶数。\n",
    "        :max_seq_len: 模型能处理的最大序列长度，默认2048。\n",
    "        :base: 用于计算旋转频率的基数默认10000。\n",
    "        '''\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        if dim %2 != 0:\n",
    "            raise ValueError(\"RotaryEmbedding dim must be even!\")\n",
    "        \n",
    "        # 单位旋转角度θ\n",
    "        theta = 1 / (base ** (torch.arange(0, dim, 2) / dim)) # theta.shape = (dim//2,)\n",
    "        # m\n",
    "        m = torch.arange(max_seq_len) # m.shape = (max_seq_len,)\n",
    "        # 获得mθ\n",
    "        m_theta = torch.outer(m, theta) # m_theta.shape = (max_seq_len, dim//2)\n",
    "\n",
    "        # 计算sin和cos\n",
    "        cos = torch.cos(m_theta) # cos.shape = (max_seq_len, dim//2)\n",
    "        sin = torch.sin(m_theta) # sin.shape = (max_seq_len, dim//2)\n",
    "\n",
    "        # 下面这个操作是对应ROT矩阵@[x1, x2].T\n",
    "        cos = torch.cat((cos, cos), dim=-1) # cos.shape = (max_seq_len, dim)\n",
    "        sin = torch.cat((sin, sin), dim=-1) # sin.shape = (max_seq_len, dim)\n",
    "\n",
    "        # register_buffer能将cos和sin加入到模型的state_dict中，但是不属于parameters，所以不会被优化器更新。\n",
    "        self.register_buffer('cos_cached', cos)\n",
    "        self.register_buffer('sin_cached', sin)\n",
    "\n",
    "    @staticmethod\n",
    "    def rotate_half(x):\n",
    "        x1, x2 = torch.chunk(x, 2, dim=-1)\n",
    "        return torch.cat((-x2, x1), dim=-1) # shape = (batch_size, seq_len, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: 输入的序列，shape = (batch_size, seq_len, dim)\n",
    "        '''\n",
    "        seq_len = x.size(1)\n",
    "        # 计算sin和cos\n",
    "        cos = self.cos_cached[:seq_len] # shape = (seq_len, dim)\n",
    "        sin = self.sin_cached[:seq_len] # shape = (seq_len, dim)\n",
    "        cos = cos.unsqueeze(0) # shape = (1, seq_len, dim)\n",
    "        sin = sin.unsqueeze(0) # shape = (1, seq_len, dim)\n",
    "\n",
    "        embed = x * cos + self.rotate_half(x) * sin\n",
    "\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b29f0d",
   "metadata": {},
   "source": [
    "### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc8ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 8])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embedding_dim = 8\n",
    "\n",
    "rope = RotaryEmbedding(dim=embedding_dim, max_seq_len=seq_len)\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "k = torch.randn(batch_size, seq_len, embedding_dim)\n",
    "\n",
    "q_with_rope = rope(q)\n",
    "k_with_rope = rope(k)\n",
    "\n",
    "print(q_with_rope.shape)\n",
    "\n",
    "# 验证旋转不改变norm长度\n",
    "original_norm = torch.linalg.norm(q, dim=-1)\n",
    "rotated_norm = torch.linalg.norm(q_with_rope, dim=-1)\n",
    "\n",
    "print(torch.allclose(original_norm, rotated_norm))\n",
    "\n",
    "# 与复数版本代码进行对比\n",
    "class RotaryEmbeddingComplex(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        t = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        freqs = torch.outer(t, inv_freq)\n",
    "        self.register_buffer(\"freqs_cis\", torch.polar(torch.ones_like(freqs), freqs))\n",
    "\n",
    "    def forward(self, x, seq_start_pos=0):\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        # --- 修正开始 ---\n",
    "        # 1. 将 x 拆分为前后两半（实部和虚部）\n",
    "        x_r = x[..., : x.shape[-1] // 2]\n",
    "        x_i = x[..., x.shape[-1] // 2 :]\n",
    "        \n",
    "        # 2. 手动组合成复数\n",
    "        x_complex = torch.complex(x_r, x_i)\n",
    "        \n",
    "        # 3. 获取旋转频率\n",
    "        freqs_cis = self.freqs_cis[seq_start_pos: seq_start_pos + seq_len].unsqueeze(0)\n",
    "        \n",
    "        # 4. 应用旋转\n",
    "        x_rotated_complex = x_complex * freqs_cis\n",
    "        \n",
    "        # 5. 拆分回实数和虚数部分\n",
    "        x_rotated_r = x_rotated_complex.real\n",
    "        x_rotated_i = x_rotated_complex.imag\n",
    "        \n",
    "        # 6. 拼接回原来的形状\n",
    "        x_out = torch.cat([x_rotated_r, x_rotated_i], dim=-1)\n",
    "        # --- 修正结束 ---\n",
    "\n",
    "        return x_out.type_as(x)\n",
    "        \n",
    "rope_complex = RotaryEmbeddingComplex(dim=embedding_dim, max_seq_len=seq_len)\n",
    "\n",
    "q_with_rope_complex = rope_complex(q)\n",
    "\n",
    "print(torch.allclose(q_with_rope, q_with_rope_complex, atol=1e-6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
